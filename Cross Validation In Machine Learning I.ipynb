{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecc0938",
   "metadata": {},
   "source": [
    "<center><em>Copyright IBM</em></center>\n",
    "<center><em><b>Created By Trilok Nath</b></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c8133",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "- When a model performs well on the train set, but not on new data, the model over-fits to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8818161",
   "metadata": {},
   "source": [
    "# Train/Test Set\n",
    "- One way to overcome the overfitting issue is to split data into train and test set.\n",
    "- Train the model on train set.\n",
    "- Evaluate the model on test set.\n",
    "<img src=\"https://github.com/sharmatriloknath/Cross-Validation-in-Machine-Learning/blob/main/Images/train_test_set.png?raw=true\"/>\n",
    "\n",
    "- On common mistake in data science that we were committing in this scenario is we are trying to apply\n",
    "  different hyperparameters on the test set and select the best one.\n",
    "- But in this case there is the issue of knowledge leak.\n",
    " <img src=\"https://github.com/sharmatriloknath/Cross-Validation-in-Machine-Learning/blob/main/Images/train_test_set1.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646121e",
   "metadata": {},
   "source": [
    "# Train/Validation/Test Set\n",
    "- To Overcome the above issue subsquently divide the train set into train and validation set.\n",
    "- Train model on most of train set.\n",
    "- Test Performance on validation Set.\n",
    "- Select best model.\n",
    "- Test best model‚Äôs performance on test set.\n",
    "\n",
    "<img src=\"https://github.com/sharmatriloknath/Cross-Validation-in-Machine-Learning/blob/main/Images/train_test_validation_set.png?raw=true\"/>\n",
    "\n",
    "**By partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.**\n",
    "\n",
    "**~~A solution to this problem is a procedure called cross-validation (CV for short).~~**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7e9c",
   "metadata": {},
   "source": [
    "# Cross Valiation\n",
    "- In this technique a test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets.The following procedure is followed for each of the k ‚Äúfolds‚Äù:\n",
    "- A model is trained using  of the folds as training data;\n",
    "- The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "- The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b2ffc",
   "metadata": {},
   "source": [
    "# Types Of Cross Validations\n",
    "1. K Fold CV.\n",
    "2. Repeated K Fold CV.\n",
    "3. Leave One Out CV.\n",
    "4. Leave P Out CV.\n",
    "5. Stratified CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261a7b0",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "- KFold divides all the samples in K groups of samples, called folds, of equal sizes (if possible). \n",
    "- The prediction function is learned using (k-1) folds.\n",
    "- The Kth fold left out is used for test.\n",
    "- Repeat the above steps k times and train k number of models.\n",
    "- The above process let you with k performance values.\n",
    "- mean(k performance values) will the final performance.\n",
    "- Below Diagram is showing the k-fold cv.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18b4f1",
   "metadata": {},
   "source": [
    "# Repeated K-Fold Cross Validation\n",
    "-  Repeats K-Fold Cross-Validation, n times, each times making different data split.\n",
    "- The values of the training set are shuffled before making the split into the K fold.\n",
    "- Repeat n times:\n",
    "  Shuffle data ÔÉ® K-Fold CV\n",
    "- K √ó n performance metrics.\n",
    "- There could be overlap between the tests sets in different repeats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7978a",
   "metadata": {},
   "source": [
    "# Leave One Out Cross Valiation.\n",
    "- LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for n samples, we have n different training sets and n different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a73080",
   "metadata": {},
   "source": [
    "# Leave P Out Cross Validation\n",
    "- Leaves out all possible subsets of p observations.\n",
    "- For n observations, this produces ùëõ permutation ùëù train-test pairs.\n",
    "- There is overlap the different test sets.\n",
    "- We have bigger validation sets --> better measure of performance (than LOO).\n",
    "- Very computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568fd36c",
   "metadata": {},
   "source": [
    "# Stratified Cross Validation.\n",
    "- StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "\n",
    "- Only for classification.\n",
    "- Procedure identical to K-fold Cross-Validation.\n",
    "- Ensures that each fold has a similar proportion of observations of each class.\n",
    "- Useful with (very) imbalanced datasets\n",
    "- K performance metrics\n",
    "- No overlap of test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db3d80",
   "metadata": {},
   "source": [
    "# Important Points\n",
    "- Generally use K-fold cross-validation with K equals 5 or 10.\n",
    "- Use Stratified K-fold if target class is imbalanced.\n",
    "- If K is too small, the error estimate is pessimistically biased because of the difference in training-set size between the original dataset and the cross\u0002validation datasets.\n",
    "- Leave-one-out cross-validation works well for estimating continuous error functions (e.g., mean squared error), but it may perform poorly for discontinuous error functions, (e.g., number of misclassified cases, precision and recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db019c6",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
